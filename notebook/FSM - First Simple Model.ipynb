{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick reminder, my capstone is looking for negative/hate comments in the chats of Twitch streams. I've done some preprocessing in a seperate notebook (titled Merge_datasets) and brought in that saved dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import recall_score, accuracy_score, make_scorer, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "#Reads in data and drops nulls\n",
    "df = pd.read_csv('../data/small_merged_chats')\n",
    "sw = stopwords.words('english')\n",
    "df = df.dropna(subset = ['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I'm using, while cleaned from the original, had some non-human messages I didn't catch until looking at the tokens. The cell below gets rid of chats that come from these bots using regex (they are mostly ad bots so they're advertising links to patreon/discord). There are also different colored/varaitions of emojis used on Twitch like pog vs. pogChamp or kappa vs. kappaRainbow that convey essentially the same meaning. In an effort to trim the data down the `emoji_shorten` function finds the most popular emojis that have a lot of variations and truncates them all to the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of chats with links (often promo/not real messages)\n",
    "def ad(chat):\n",
    "    result = False\n",
    "    result = bool(re.search(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', chat))\n",
    "    result = bool(re.search(r'http\\S+', chat))\n",
    "    return result\n",
    "\n",
    "#Makes ad column and gets rid of any ad messages\n",
    "df['is_ad'] = df['body'].apply(ad)\n",
    "df = df[df['is_ad'] == False]\n",
    "\n",
    "#Changes popular emojis to one type so no variants ie) pog + pogChamp -> pog + pog\n",
    "def emoji_shorten(chat):\n",
    "    chat = re.sub(r'(?i) \\bpog(\\w)*\\b |\\bpog(\\w)*\\b', '', chat)\n",
    "    chat = re.sub(r'(?i) \\blul(\\w)*\\b |\\blul(\\w)*\\b', '', chat)\n",
    "    chat = re.sub(r'(?i) \\bkappa(\\w)*\\b |\\bkappa(\\w)*\\b', '', chat)\n",
    "    return chat\n",
    "\n",
    "#Creates new column with emojis shortened to simple form\n",
    "df['chats'] = df.body.apply(lambda x: emoji_shorten(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER, while great with social media chats and slang, doesn't have Twitch specific lingo. In an effort to get more accurate labels I've added some of the popular tokens that I noticed are missing from VADER's vocabulary. The polarity of the sentiment is calculated using a study from [here](https://dl.acm.org/doi/10.1145/3365523) in an effort to get an accurate number rather than just my own intuition. I then add this to the analyzer's vocabulary when I create the labels in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to add to VADER\n",
    "#Values calculated from: https://dl.acm.org/doi/10.1145/3365523\n",
    "new_words = {\n",
    "    'noice': 1.8,\n",
    "    'scum': -2.0,\n",
    "    'kap': 0.5,\n",
    "    'kappa': 0.5,\n",
    "    'lul': 1.8,\n",
    "    'omegalol': 1.8,\n",
    "    'strats': 2.0,\n",
    "    'rekt': 0,\n",
    "    'owo': 1.0,\n",
    "    'tweaker': -2.3,\n",
    "    'pog': 2.8,\n",
    "    'pag': 2.8,\n",
    "    'incel': -3.1,\n",
    "    'tilted': -0.7,\n",
    "    'feelsbadman': -2.6,\n",
    "    'feelsgoodman': 3.7,\n",
    "    'trash': -2.0,\n",
    "    'rip': -1.2,\n",
    "    'ez': 1.9,\n",
    "    'clap': 2.7,\n",
    "    'hyperbruh': -0.6,\n",
    "    'f': 0.5,\n",
    "    'F': 0.5,\n",
    "    'discord': 0,\n",
    "    'PJSalt': -1.2,\n",
    "    'Kreygasm': 2.8,\n",
    "    'kreygasm': 2.8,\n",
    "    'homo': -3.5,\n",
    "    'clip': 0.5,\n",
    "    'rAcIsM': -3.1,\n",
    "    'based': 2.0,\n",
    "    'Based': 2.0,\n",
    "    'PepeHands': -1.7,\n",
    "    'WutFace': -1.7,\n",
    "    'FailFish': -2.0,\n",
    "    'BabyRage': -1.6,\n",
    "    'ANELE': -0.8,\n",
    "    'haHAA': -0.5,\n",
    "    'ResidentSleeper': -1.2,\n",
    "    'cmonBruh': -1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates labels for data\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for chat in df.chats:\n",
    "    analysis = TextBlob(chat)\n",
    "    vad = SentimentIntensityAnalyzer()\n",
    "    #New words get added to vocabulary\n",
    "    vad.lexicon.update(new_words)\n",
    "    score = vad.polarity_scores(chat)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    " \n",
    "    if neg > pos:\n",
    "        negative_list.append(chat)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(chat)\n",
    "        positive += 1\n",
    " \n",
    "    elif pos == neg:\n",
    "        neutral_list.append(chat)\n",
    "        neutral += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just functions I'm using to tokenize my chat data. I'm again removing any non-ASCII characters to avoid crashes or any emojis encodings that lost characters then tokenizing the chats. These are then stripped of any unwanted puntuation and lemmatized. Notably I've noticed a good number of mispellings of words that may be adding extra noise, but I haven't found a good way to spellcheck without correcting non-words like the Twitch emoticons as well, so that's a project on hold I'm hoping to implement into the tokenizer function at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['?', '!', '@', ',', '.']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Removes non-ASCII characters (aka emojis that cant be converted to original symbol)\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "\n",
    "def chat_tokenizer(doc, stop_words = sw):\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer as chats like tweets\n",
    "    chat_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = chat_token.tokenize(doc)\n",
    "    #Strips extra puntuation I don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there's class imbalance, so I've been SMOTEing the minority in my models. The actual simple model is down below. I opted to start with Bayes as it tends to do well with text classification. However, the metric I'm using is recall, as I'm trying to limit the number of missed hate chats so I'm weighting the model against these false negatives. I've found with Bayes though the validation recall score is always much higher than the training, so I'm also using accuracy as a metric for now to be able to tell if the model is overfit/if there is actually a gap in performance between training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.887196\n",
       "1    0.112804\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hate or negative chat = 1 / neutral + positive = 0\n",
    "df[\"label\"] = np.where(df[\"chats\"].isin(negative_list), 1, 0)\n",
    "df.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Recall: 0.5652359295054008\n",
      "Training Accuracy: 0.9109250243427459\n",
      "Validation Recall:0.8309753921199263\n",
      "Validation Accuracy:0.8851282051282052\n"
     ]
    }
   ],
   "source": [
    "#Creates features and target then performs train test split\n",
    "y = df['label']\n",
    "X = df['chats']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Pipeline for processing and fitting model\n",
    "mnb_cv = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(lowercase = False, tokenizer = chat_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training score\n",
    "mnb_cv.fit(X_train, y_train)\n",
    "preds = mnb_cv.predict(X_train)\n",
    "print(\"Training Recall:\", recall_score(preds, y_train))\n",
    "print(\"Training Accuracy:\", mnb_cv.score(X_train, y_train))\n",
    "#Cross validates model and prints average result\n",
    "scoring = {\n",
    "    'acc': make_scorer(accuracy_score),\n",
    "    'rec': 'recall'\n",
    "}\n",
    "scores = cross_validate(mnb_cv, X_train, y_train, cv = 5, scoring = scoring)\n",
    "print(\"Validation Recall:\" + str(np.mean(scores['test_rec'])))\n",
    "print(\"Validation Accuracy:\" + str(np.mean(scores['test_acc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Models notebook in this same folder has a bunch of different iterations/some more feature engineering and exploring if there's anything I missed here you might have questions about. I opted to put the first simple model in this notebook though since the Models one is... messy and hard to navigate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
